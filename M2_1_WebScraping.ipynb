{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python para data science na prática"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Na era digital, a quantidade e a variedade de dados disponíveis na web crescem de maneira constante. Empresas precisam desses dados para tomar decisões, especialmente com o crescimento das ferramentas de Inteligência Artificial (IA), de aprendizado de máquina, que demandam grandes volumes de dados para treinamento. Embora muitos dados estejam acessíveis por APIs, diversos dados valiosos ainda só podem ser obtidos por meio de web scraping.\n",
    "\n",
    "O Web Scraping é amplamente utilizado por empresas e pesquisadores para diversas finalidades, como:\n",
    "- Monitorar concorrentes e analisar tendências de mercado.\n",
    "- Coletar dados para estudos acadêmicos e projetos de pesquisa.\n",
    "- Desenvolver aplicações de inteligência artificial e machine learning.\n",
    "\n",
    "O Web scraping revoluciona a forma de como interagir e extrair informações da internet. \n",
    "\n",
    "## Aplicações\n",
    "- Análise de mercado: Empresas utilizam para monitorar preços e tendências.\n",
    "- Pesquisa acadêmica: Coletando dados em larga escala para estudos.\n",
    "- Desenvolvimento de IA: Treinando modelos com dados retirados da web.\n",
    "- Monitoramento de mídia: Acompanhar notícias e menções de marcas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é Web scraping?\n",
    "\n",
    "A coleta automatizada de dados da Internet é quase tão antiga quanto a própria Internet. Embora web scraping não seja um termo novo, nos últimos anos a prática era mais comumente conhecida como screen scraping, mineração de dados, web harvesting ou algo semelhante.\n",
    "\n",
    "**Web scraping** é o processo de extração automática de dados de websites. Utilizando scripts ou ferramentas específicas, é possível coletar grandes volumes de informações de páginas da web, transformando dados não estruturados em dados estruturados e organizados para análise posterior.\n",
    "\n",
    "O web scraping, geralmente é feito escrevendo um programa automatizado que consulta um servidor web, solicita dados (geralmente na forma de HTML e outros arquivos que compõem as páginas web) e depois se analisam esses dados para extrair as informações necessárias. \n",
    "\n",
    "As informações são consolidadas e armazenadas em um dataset que servirá para analisar e tomar as decisões adequadas ao caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O processo de Web Scraping\n",
    "\n",
    "Os raspadores da Web operam da seguinte forma:\n",
    "\n",
    "1. Um usuário ou um aplicativo insere uma URL no web scraper.\n",
    "2. O web scraper coleta todo o conteúdo das páginas da web ou apenas as informações específicas para as quais foi configurado.\n",
    "3. Em seguida, o web scraper processa os dados coletados e os formata em arquivos CSV, Excel ou JSON, que podem ser utilizados por usuários ou aplicativos.\n",
    "\n",
    "Embora pareça um processo simples, a extração real de dados pode ser bastante complexa, especialmente se o objetivo for extrair informações específicas. Além disso, dependendo do tamanho do site copiado, o processo pode ser demorado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ferramentas de Web Scraping in Python\n",
    "\n",
    "Uma ferramenta de web scraping é uma biblioteca, software ou serviço que simplifica e facilita a extração automática de dados de um site. Entre estas bibliotecas temos as seguintes:\n",
    "- Requests - [Requests](https://pypi.org/project/requests/)\n",
    "- BeautifulSoup - [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- Scrapy - [Scrapy](https://scrapy.org/)\n",
    "- Selenium - [Selenium](https://www.selenium.dev/)\n",
    "- Urllib3 - [urllib3](https://pypi.org/project/urllib3/)\n",
    "- Lxml - [Lxml](https://pypi.org/project/lxml/)\n",
    "- MechanicalSoup - [MechanicalSoup](https://mechanicalsoup.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requests\n",
    "\n",
    "Requests é uma biblioteca de web scraping popular em Python que facilita a geração de múltiplas requisições HTTP. Isso é extremamente útil para web scraping, já que a etapa primária em qualquer processo de web scraping é enviar requisições HTTP ao servidor do site para extrair os dados exibidos na página da web desejada.\n",
    "\n",
    "- Requests suporta a API restful e suas funcionalidades (PUT, GET, DELETE e POST) e oferece ampla documentação.\n",
    "- Esta biblioteca suporta tratamento de erros, incluindo Connection Error, Timeout, TooManyRedirect, Response.raise_for_status, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcionamento da bibliotecas Request  \n",
    "\n",
    "A primeira coisa que precisamos fazer para examinar uma página da web é fazer o download dela. Podemos fazer o download de páginas usando a biblioteca **requests** do Python. \n",
    "\n",
    "A biblioteca requests fará uma solicitação GET para um servidor da web, que fará o download do conteúdo HTML de uma determinada página da Web. Existem vários tipos diferentes de solicitações que podemos fazer usando requests, GET é apenas uma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testando o funcionamento de requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test - import of the library request\n",
    "import requests                # Biblioteca Requests\n",
    "from pprint import pprint      # imprime resultados\n",
    "# Testing output Requests\n",
    "#response = requests.get('http://dusp.mit.edu/people')\n",
    "#response = requests.get('https://larepublica.pe/')\n",
    "response = requests.get('https://www.pro-football-reference.com/years/2024/draft.htm')\n",
    "print(response.text)                         # Print o output da página HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup\n",
    "\n",
    "> A biblioteca BeautifulSoup é amplamente usada para analisar documentos HTML, XML ou outras linguagens de marcação  e extrair dados e gerar arquivos de dados mais claramente estruturados, isso é o que se chama de ***Web Scraping ou Raspagem de Dados***. \n",
    "\n",
    "![wst](images/web_scrap.png)\n",
    "\n",
    "> Em geral se usa essa técnica  para coletar informações de um website que não possuia API. Para compreenssão de alguns conceitos nessa aula é importante ter conhecimentos mínimos de HTML tree structure.\n",
    "\n",
    "#### Leituras complementares\n",
    "> -  Básico de HTML- BeautifulSoap Aqui --> [Guia para iniciantes](https://www.vooo.pro/insights/guia-para-iniciantes-de-web-scraping-em-python-usando-beautifulsoup/)\n",
    "> -  Documentação oficial do BeautifulSoap --> [BeatifulSoap](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcionamento da biblioteca BeautifulSoap \n",
    "\n",
    "A biblioteca **BeautifulSoup** é usada para analisar o documento e extrair os textos a partir do elementos do HTML. \n",
    "\n",
    "Primeiro, precisamos importar a biblioteca e criar uma instância da classe BeautifulSoup para analisar um documento pretendido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test - import of the library beautifulsoup\n",
    "import pandas as pd                       # dataframes\n",
    "import numpy as np                        # numeros\n",
    "import seaborn as sns                     # graficos\n",
    "import matplotlib.pyplot as plt           # graficos\n",
    "import statsmodels.formula.api as smf     # estatistica\n",
    "import bs4                                # Biblioteca BeautifulSoap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testando BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os próximos passos são testes de  outputs de uma requisição do site que desejamos \"raspar\" os dados. \n",
    "\n",
    "Utilizaremos  **http://dusp.mit.edu/people**  para criar um arquivo texto (CSV) com  dados livre de formatação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.pro-football-reference.com/years/2024/draft.htm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testando BeautifulSoup\n",
    "\n",
    "##### Inspecionar um WebSite \n",
    "\n",
    "Os browsers já têm incorporado um \"inspecionador\", que é ativado ao clicar sobre uma página com botão direito do mouse.\n",
    "\n",
    "![Inspecionar](images/bs.png)\n",
    "\n",
    "\n",
    "##### Prettify \n",
    "- Método do BS  para visualizar a estrutura separada da página HTML\n",
    "- Em linhas gerais o código a seguir irá analisar response.text criado pelo objeto BeautifulSoup e atribuido-o a soup. \n",
    "\n",
    "O argumento html.parser indica que queremos fazer a análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing output BS\n",
    "soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "print(soup.prettify())                        # Print o output usando a função 'prettify' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Navegando na estrutura de dados gerada pelo Prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the title \n",
    "# Access the title element\n",
    "#soup.titlelement\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the content of the title element\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data in the first 'p' tag\n",
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data in the first 'a' tag\n",
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all links in the document (note it returns an array)\n",
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all links in the document (note it returns an array)\n",
    "grouping = soup.find_all(id=\"info\")\n",
    "pprint(grouping[0])\n",
    "#print(grouping[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve elements by class equal to link using the attributes argument\n",
    "people = soup.find_all('div', class_='years')\n",
    "pprint(people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar os dados usando *print()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draft2024 = pd.read_html(URL, header=1)[0] \n",
    "draft2024.loc[draft2024[\"DrAV\"].isnull(), \"DrAV\"] = 0\n",
    "print(draft2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para extrair vários anos (por exemplo, 2000 a 2024), se pode usar um loop for simples – o que geralmente é possível devido a alterações sistemáticas nos dados. A experimentação é fundamental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando os Dados em Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A maneira mais fácil de acessar elementos (dados) da página é gravá-los em um arquivo temporário e manipulá-los e salvá-los como objetos. \n",
    "\n",
    "Observe que os dados da página a ser raspada são organizados em \"counties\" e várias colunas com números. Logo, salvá-los em arrays é maneira mais lógica e fácil de trabalhar com os dados futuramente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create file\n",
    "import os.path \n",
    "# \n",
    "file_name = \"data/draft2024.csv\"  \n",
    "\n",
    "if not os.path.isfile(file_name):\n",
    "     # url\n",
    "     #UTL = \"https://www.pro-football-reference.com/years/2024/draft.htm\" \n",
    "     # leituar da web (url)\n",
    "     draft_py = pd.read_html(URL, header=1)[0] \n",
    "     # colunas requeridas \n",
    "     # Tm = times\n",
    "     # SDG = San Diego\n",
    "     # OAK = Oaklan\n",
    "     # STL = St Louis\n",
    "     conditions = [(draft_py.Tm == \"SDG\"), \n",
    "                   (draft_py.Tm == \"OAK\"), \n",
    "                   (draft_py.Tm == \"STL\"),]  \n",
    "     # LAC = the Chargers moved to Los Angeles from San Diego \n",
    "     # LV = the Raiders moved from Oakland to Las Vegas \n",
    "     # LAR = the Rams moved from St. Louis to Los Angeles\n",
    "     choices = [\"LAC\", \"LV\", \"LAR\"]  \n",
    "     # select\n",
    "     draft_py[\"Tm\"] = np.select(conditions, choices, default = draft_py.Tm)\n",
    "     # cria coluna DrAV =0 \n",
    "     draft_py.loc[draft_py[\"DrAV\"].isnull(), \"DrAV\"] = 0 \n",
    "     # gravar em dataframe em csv\n",
    "     df = pd.DataFrame(draft_py)\n",
    "     df.to_csv(file_name, index=False)\n",
    "     #draft_py.to_csv(file_name, index=False) \n",
    "else:  \n",
    "    #draft_py = pd.read_csv(file_name, index=False) \n",
    "    #draft_py.loc[draft_py[\"DrAV\"].isnull(), \"DrAV\"] = 0\n",
    "    df = pd.read_csv(file_name, index=False) \n",
    "    df.loc[df[\"DrAV\"].isnull(), \"DrAV\"] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset dimentions\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_NFL = \"data/draf_NFL.csv\"\n",
    "url1 = \"https://www.pro-football-reference.com/years/\"\n",
    "url2 = \"/draft.htm\"\n",
    "#\n",
    "if os.path.isfile(file_NFL):\n",
    "    draft_py = pd.read_csv(file_NFL)\n",
    "else:\n",
    "    draft_py = pd.DataFrame()\n",
    "    for i in range(2000, 2022 + 1):\n",
    "        url = url1+str(i)+url2\n",
    "        web_data = pd.read_html(url, header=1)[0]\n",
    "        web_data[\"Season\"] = i\n",
    "        web_data = web_data.query('Tm != \"Tm\"')\n",
    "        draft_py = pd.concat([draft_py, web_data])\n",
    "    draft_py.to_csv(file_NFL)\n",
    "#\n",
    "draft_py.reset_index(drop=True, inplace=True)\n",
    "\n",
    "## rename teams\n",
    "# the Chargers moved to Los Angeles from San Diego\n",
    "# the Raiders moved from Oakland to Las Vegas\n",
    "# the Rams moved from St. Louis to Los Angeles\n",
    "\n",
    "conditions = [\n",
    "    (draft_py.Tm == \"SDG\"),\n",
    "    (draft_py.Tm == \"OAK\"),\n",
    "    (draft_py.Tm == \"STL\"),\n",
    "]\n",
    "choices = [\"LAC\", \"LVR\", \"LAR\"]\n",
    "draft_py[\"Tm\"] = np.select(conditions, choices, default=draft_py.Tm)\n",
    "\n",
    "## replace missing DrAV with 0\n",
    "draft_py.loc[draft_py[\"DrAV\"].isnull(), \"DrAV\"] = 0\n",
    "draft_py.to_csv(\"data/data_NFL.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape\n",
    "draft_py.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando registros\n",
    "draft_py.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando e executando o arquivo de script do BeatutifuSoap (.py) para raspagem de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obeserve que podemos fazer a raspagem executando diretamente um script python através da linha de comandos, pois muitas vezes é um processo demorado. No caso dessa aula, dividimos o script em duas céluas para serem execuradas ao vivo. \n",
    "\n",
    "Caso deseje gerar um script .py, copie o conteúdo das duas próximas céluas em um único arquivo e execute diretamente no Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opcionalmente pode ser executado dentro do Jupyter\n",
    "import requests\n",
    "import bs4\n",
    "from io import StringIO\n",
    "\n",
    "# load and get the page from the website\n",
    "url_draft = \"http://pt.wikipedia.org/wiki/Lista_de_unidades_federativas_do_Brasil_por_popula%C3%A7%C3%A3o\"\n",
    "page = requests.get(url_draft)\n",
    "# create the soup\n",
    "soup = bs4.BeautifulSoup(page.text, \"html.parser\")\n",
    "# find all the tables\n",
    "tabela = soup.find('span', 'mw-editsection').find_next('table')\n",
    "#print(tabela.get_text())\n",
    "#print(len(tabela))\n",
    "# convertir html to objeto StringIO\n",
    "tabela_io = StringIO(str(tabela))\n",
    "# Transformando a tabela em um DataFrame\n",
    "df = pd.read_html(tabela_io)[0]\n",
    "# gravando\n",
    "#df.to_csv('data/uf_pop.csv',sep=',',index=False)\n",
    "#visualiza\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gravando dados em um arquivo texto (csv) usando um loop simples. A Lógica é mais ou menos a mesma em vários casos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo prático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test - import of the library request\n",
    "import requests                # Biblioteca Requests\n",
    "from pprint import pprint      # imprime resultados\n",
    "# Testing output Requests\n",
    "#response = requests.get('http://dusp.mit.edu/people')\n",
    "re_pe = requests.get('https://larepublica.pe/')\n",
    "#response = requests.get('https://www.pro-football-reference.com/years/2024/draft.htm')\n",
    "print(re_pe.text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o BeautifulSoap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test - import of the library beautifulsoup\n",
    "import pandas as pd                       # dataframes\n",
    "import numpy as np                        # numeros\n",
    "import seaborn as sns                     # graficos\n",
    "import matplotlib.pyplot as plt           # graficos\n",
    "import statsmodels.formula.api as smf     # estatistica\n",
    "import bs4    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing output BS\n",
    "html_re = bs4.BeautifulSoup(re_pe.text, \"html.parser\")\n",
    "print(html_re.prettify())                        # Print o output usando a função 'prettify' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the title \n",
    "html_re.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acesar outros elementos \n",
    "html_re.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_text = html_re.find('body')\n",
    "pprint(body_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acessa links\n",
    "links = body_text.find_all('h1')\n",
    "pprint(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela = html_re.find_all('<section>')\n",
    "pprint(tabela)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Python para data science &copy; Jorge Zavaleta, 2024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
